{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vectorizer as vr\n",
    "import cluster as clst\n",
    "import analyze as anlz\n",
    "import warnings\n",
    "import argparse\n",
    "from LM_vectorizer import batchify, get_docs_repr\n",
    "import create_vec\n",
    "\n",
    "# import data\n",
    "from dataloader import *\n",
    "%matplotlib inline\n",
    "def create_dir(dir):\n",
    "    print(dir)\n",
    "    print(os.getcwd())\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    return os.path.abspath(dir)\n",
    "# from LM_hagai import repackage_hidden\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import our libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Dataloader(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "labels_names = [corpus.target_names[x] for x in corpus.labels]\n",
    "super_class_labels_names = corpus.super_class_labels_by_name\n",
    "super_class_labels = corpus.super_class_labels\n",
    "os.chdir('/home/lab/vgilad/PycharmProjects/lstm_ds_project/18_12')\n",
    "num_of_documents = len(corpus.only_encoded_docs)\n",
    "labels = corpus.labels\n",
    "pp_docs = [' '.join([corpus.decoder[str(w)] for w in doc]) for doc in corpus.only_encoded_docs]\n",
    "emails = pp_docs\n",
    "# emails = corpus.raw_data\n",
    "# labels = corpus.raw_labels\n",
    "example_idx = 0\n",
    "number_of_labels = 20  # TODO magic number\n",
    "number_of_labels_super = 6  # TODO magic number\n",
    "\n",
    "max_df = 1.  # 0.05\n",
    "min_df = 0.  # 1e-4\n",
    "all_k = [20, 50] #, 200]\n",
    "all_vect = [vect_tfidf, vect_w2v, vect_bow]\n",
    "assert (len(emails) == len(labels))\n",
    "num_of_documents = len(emails)\n",
    "\n",
    "\n",
    "print(corpus.raw_data[corpus.encoded_docs[example_idx][0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wondering car saw sports car looked late 60s early 70s doors small addition front bumper separate rest body model name engine specs production car history whatever info car\n"
     ]
    }
   ],
   "source": [
    "print(pp_docs[example_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Params(vect_tfidf, clust_kneams, aff_euclidean, link_ward, min_df, max_df, 20, num_of_documents,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select vectorizing method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (param.vectorizing == vect_bow):\n",
    "    # BOW\n",
    "    cv, tokenized_emails = vr.tokenizer(emails, param.min_df, param.max_df)\n",
    "    # {k: v for k, v in zip(cv.get_feature_names(), sample_vec.toarray()[0]) if v > 0}\n",
    "    # to get vocabulary names use: cv.get_feature_names()\n",
    "    voc_names = cv.get_feature_names()\n",
    "    # to get BOW for each mail use: tokenized_emails.toarray()\n",
    "    emails_representation = tokenized_emails.toarray()\n",
    "elif (param.vectorizing == vect_tfidf):\n",
    "    # TF-IDF\n",
    "    tf_idf = vr.tf_idf(emails, param.min_df, param.max_df)\n",
    "    emails_representation = tf_idf.toarray()\n",
    "elif (param.vectorizing == vect_w2v):\n",
    "    # Word2Vec\n",
    "    emails_representation = vr.BOW_w2v(emails,\"w2v.pickle\")\n",
    "elif param.vectorizing == vect_LM:\n",
    "    with open(args.checkpoint, 'rb') as f:\n",
    "        if args.cuda:\n",
    "            model = torch.load(f).to(device)\n",
    "        else:\n",
    "            model = torch.load(f, map_location='cpu').to(device)\n",
    "    model.eval()\n",
    "    data = batchify(corpus.only_encoded_docs, 1, device)\n",
    "    emails_representation = get_docs_repr(model, data)\n",
    "elif param.vectorizing == vect_gilad:\n",
    "    # data = batchify(corpus.only_encoded_docs, 1, device)\n",
    "    import opts\n",
    "\n",
    "    # take only test documents (fist 1000 are val last 2000 are train)\n",
    "    ntokens = len(corpus.decoder)\n",
    "    num_of_documents = len(corpus.only_encoded_docs[1000:-2000])\n",
    "    labels = corpus.labels[1000:-2000]\n",
    "    pp_docs = [' '.join([corpus.decoder[str(w)] for w in doc]) for doc in\n",
    "               corpus.only_encoded_docs]\n",
    "    emails = pp_docs[1000:-2000]\n",
    "    super_class_labels = corpus.super_class_labels[1000:-2000]\n",
    "    super_class_labels_names = corpus.super_class_labels_by_name[1000:-2000]\n",
    "    labels_names = [corpus.target_names[x] for x in labels]\n",
    "\n",
    "    opt = opts.parse_opt()\n",
    "    emails_representation, labels_not_used = create_vec.create_vec(opt)  # get numpy matrix\n",
    "\n",
    "    # emails_representation = emails_representation[1000:-2000]\n",
    "else:\n",
    "    raise ValueError('vectorizing is not supported with: ' + param.clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute tsne with perplexity 30 and seed 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "several sets except to list of pathes",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d2bce70a9b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manlz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memails_representation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabels_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuper_class_labels_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/lstm_ds_project/analyze.py\u001b[0m in \u001b[0;36mplot_tsne\u001b[0;34m(high_dim_repr, labels, seed, perplexity, alpha, fpath)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'several sets except to list of pathes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tsne by several sets of labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: several sets except to list of pathes"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = anlz.genrate_tsne(emails_representation, seed=4, perplexity=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlz.plot_tsne_df(df, (labels_names, super_class_labels_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmeans sum of distanses = 9740.223330\n"
     ]
    }
   ],
   "source": [
    "        if (param.clustering == clust_kneams):\n",
    "            if (param.affine == 'cosine'):\n",
    "                # k-means cosine dist\n",
    "                clusters, kmns_class = clst.kmeans_cosine_dist(emails_representation, param.k)\n",
    "            elif (param.affine == 'euclidean'):  # k-means euclidean\n",
    "                clusters, kmns_class = clst.kmeans(emails_representation, param.k)\n",
    "                print('kmeans sum of distanses = %f' % kmns_class.inertia_)\n",
    "            else:\n",
    "                raise ValueError('kmenas is not supported with affinity: ' + param.affine)\n",
    "        elif (param.clustering == clust_hirarchical):\n",
    "            # hirarchical\n",
    "            (clusters, _) = clst.hirarchical(emails_representation, param.k, aff=param.affine, link=param.linkage)\n",
    "        else:\n",
    "            raise ValueError('clustering is not supported with: ' + param.clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analayze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "analyze_clustering() got an unexpected keyword argument 'calc_linkage'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b7e71e2113c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults_super_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manlz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuper_class_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_labels_super\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_linkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy = %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy by super_class= %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mresults_super_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: analyze_clustering() got an unexpected keyword argument 'calc_linkage'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "results,results_super_class = anlz.analyze_clustering(labels, clusters, number_of_labels,super_class_labels, number_of_labels_super, calc_linkage=False)\n",
    "print('accuracy = %f' % results.get_list()[0])\n",
    "print('accuracy by super_class= %f' % results_super_class.get_list()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
